{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Perceptron, uma unidade fundamental em redes neurais, possui camadas: entrada (input) para dados, ocultas (hidden layers) e saída (output) para resultados.\n",
    "\n",
    "Seu aprendizado ocorre em duas etapas: Propagação Direta (Forward Propagation) e Retropropagação (Backward Propagation):\n",
    "\n",
    "Na Propagação Direta, cada neurônio recebe valores dos anteriores, ponderados pelos pesos e somados. Isso passa por função de ativação não linear. É vital para aprendizado, especialmente em tarefas complexas.\n",
    "\n",
    "Este processo se repete para todos os neurônios nas camadas até a saída gerar resultados.\n",
    "\n",
    "O resultado é avaliado por função de custo, minimizada para alto desempenho. Minimização envolve descida de gradiente, ajustando parâmetros.\n",
    "\n",
    "O aprendizado está na Retropropagação, onde pesos e bias atualizam com derivadas em relação ao custo. Atualização segue:\n",
    "\n",
    "Parâmetro atualizado = Parâmetro - derivada * learning rate\n",
    "\n",
    "Learning rate, 0 a 1, guia taxa de movimento dos parâmetros rumo a mínimo. Escolher correto é vital, impacta eficiência da otimização na descida de gradiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263221</th>\n",
       "      <td>160848.0</td>\n",
       "      <td>1.812322</td>\n",
       "      <td>-0.805928</td>\n",
       "      <td>-1.493664</td>\n",
       "      <td>0.267995</td>\n",
       "      <td>0.098902</td>\n",
       "      <td>0.230844</td>\n",
       "      <td>-0.192742</td>\n",
       "      <td>0.084907</td>\n",
       "      <td>0.831115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005528</td>\n",
       "      <td>-0.258719</td>\n",
       "      <td>-0.052341</td>\n",
       "      <td>-1.462074</td>\n",
       "      <td>-0.166718</td>\n",
       "      <td>0.288492</td>\n",
       "      <td>-0.077725</td>\n",
       "      <td>-0.058050</td>\n",
       "      <td>139.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>11574.0</td>\n",
       "      <td>-0.470738</td>\n",
       "      <td>0.371406</td>\n",
       "      <td>1.415019</td>\n",
       "      <td>0.417889</td>\n",
       "      <td>-0.767441</td>\n",
       "      <td>-0.253796</td>\n",
       "      <td>0.668798</td>\n",
       "      <td>-0.248302</td>\n",
       "      <td>1.495213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120729</td>\n",
       "      <td>0.107825</td>\n",
       "      <td>0.028778</td>\n",
       "      <td>0.421878</td>\n",
       "      <td>-0.585630</td>\n",
       "      <td>1.099973</td>\n",
       "      <td>-0.022837</td>\n",
       "      <td>0.146680</td>\n",
       "      <td>138.22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108414</th>\n",
       "      <td>70886.0</td>\n",
       "      <td>-1.066721</td>\n",
       "      <td>0.408432</td>\n",
       "      <td>1.420904</td>\n",
       "      <td>0.890988</td>\n",
       "      <td>0.468988</td>\n",
       "      <td>-0.391520</td>\n",
       "      <td>1.101967</td>\n",
       "      <td>-0.406340</td>\n",
       "      <td>-0.746193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.378915</td>\n",
       "      <td>-0.797446</td>\n",
       "      <td>0.283403</td>\n",
       "      <td>0.081642</td>\n",
       "      <td>-0.038337</td>\n",
       "      <td>0.417571</td>\n",
       "      <td>-0.347904</td>\n",
       "      <td>-0.085699</td>\n",
       "      <td>69.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217698</th>\n",
       "      <td>140994.0</td>\n",
       "      <td>2.113442</td>\n",
       "      <td>0.050843</td>\n",
       "      <td>-1.500615</td>\n",
       "      <td>0.268930</td>\n",
       "      <td>0.634220</td>\n",
       "      <td>-0.296820</td>\n",
       "      <td>0.223754</td>\n",
       "      <td>-0.262514</td>\n",
       "      <td>0.347780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320032</td>\n",
       "      <td>-0.761359</td>\n",
       "      <td>0.256558</td>\n",
       "      <td>0.057941</td>\n",
       "      <td>-0.095130</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>-0.054444</td>\n",
       "      <td>-0.054314</td>\n",
       "      <td>4.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213970</th>\n",
       "      <td>139472.0</td>\n",
       "      <td>-0.687378</td>\n",
       "      <td>0.877389</td>\n",
       "      <td>0.725428</td>\n",
       "      <td>-0.507154</td>\n",
       "      <td>0.277876</td>\n",
       "      <td>0.427389</td>\n",
       "      <td>-0.099900</td>\n",
       "      <td>0.475242</td>\n",
       "      <td>0.241289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356397</td>\n",
       "      <td>1.119852</td>\n",
       "      <td>-0.253242</td>\n",
       "      <td>0.112419</td>\n",
       "      <td>-0.089989</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>-0.038203</td>\n",
       "      <td>0.110873</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "263221  160848.0  1.812322 -0.805928 -1.493664  0.267995  0.098902  0.230844   \n",
       "8585     11574.0 -0.470738  0.371406  1.415019  0.417889 -0.767441 -0.253796   \n",
       "108414   70886.0 -1.066721  0.408432  1.420904  0.890988  0.468988 -0.391520   \n",
       "217698  140994.0  2.113442  0.050843 -1.500615  0.268930  0.634220 -0.296820   \n",
       "213970  139472.0 -0.687378  0.877389  0.725428 -0.507154  0.277876  0.427389   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "263221 -0.192742  0.084907  0.831115  ... -0.005528 -0.258719 -0.052341   \n",
       "8585    0.668798 -0.248302  1.495213  ... -0.120729  0.107825  0.028778   \n",
       "108414  1.101967 -0.406340 -0.746193  ... -0.378915 -0.797446  0.283403   \n",
       "217698  0.223754 -0.262514  0.347780  ... -0.320032 -0.761359  0.256558   \n",
       "213970 -0.099900  0.475242  0.241289  ...  0.356397  1.119852 -0.253242   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "263221 -1.462074 -0.166718  0.288492 -0.077725 -0.058050  139.95      0  \n",
       "8585    0.421878 -0.585630  1.099973 -0.022837  0.146680  138.22      0  \n",
       "108414  0.081642 -0.038337  0.417571 -0.347904 -0.085699   69.95      0  \n",
       "217698  0.057941 -0.095130  0.022175 -0.054444 -0.054314    4.29      0  \n",
       "213970  0.112419 -0.089989 -0.126535 -0.038203  0.110873    1.55      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "df = data.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n",
    "    def __init__(self, params=None):     \n",
    "        if (params == None): \n",
    "            self.inputLayer = 30                       # Camada de Input\n",
    "            self.hiddenLayer = 10                       # Camadas Ocultas\n",
    "            self.OutputLayer = 2                       # Camadas de Output\n",
    "            self.learningRate = 0.005                  # Taxa de aprendizado\n",
    "            self.max_epochs = 600                      # Épocas\n",
    "            self.BiasHiddenValue = -1                   # Bias das Camadas Ocultas\n",
    "            self.BiasOutputValue = -1                  # Bias da Camada de Output\n",
    "            self.activation = self.ativacao['sigmoid'] # Função de Ativação\n",
    "            self.deriv = self.derivada['sigmoid']      # Derivada Função de Ativação\n",
    "        else: \n",
    "            self.inputLayer = params['InputLayer']\n",
    "            self.hiddenLayer = params['HiddenLayer']\n",
    "            self.OutputLayer = params['OutputLayer']\n",
    "            self.learningRate = params['LearningRate']\n",
    "            self.max_epochs = params['Epocas']\n",
    "            self.BiasHiddenValue = params['BiasHiddenValue']\n",
    "            self.BiasOutputValue = params['BiasOutputValue']\n",
    "            self.activation = self.ativacao[params['ActivationFunction']]\n",
    "            self.deriv = self.derivada[params['ActivationFunction']]\n",
    "        \n",
    "        #Iniciando pesos e bias aleatoriamente\n",
    "        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n",
    "        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n",
    "        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n",
    "        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n",
    "        self.classes_number = 2\n",
    "        \n",
    "    pass\n",
    "    \n",
    "    def starting_weights(self, x, y): # pesos aleatorios\n",
    "        return [[np.random.normal() for i in range(x)] for j in range(y)]\n",
    "# Funções de ativação utilizadas\n",
    "    ativacao = { \n",
    "         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n",
    "            'tanh': (lambda x: np.tanh(x)),\n",
    "            'Relu': (lambda x: x*(x > 0)),\n",
    "               }\n",
    "    derivada = {\n",
    "         'sigmoid': (lambda x: x*(1-x)),\n",
    "            'tanh': (lambda x: 1-x**2),\n",
    "            'Relu': (lambda x: 1 * (x>0))\n",
    "               }\n",
    " \n",
    "    def Backpropagation_Algorithm(self, x):\n",
    "        DELTA_output = []\n",
    "        # Erro: OutputLayer\n",
    "        ERROR_output = self.output - self.OUTPUT_L2\n",
    "        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n",
    "        \n",
    "        arrayStore = []\n",
    "        # Atualizando pesos da OutputLayer e HiddenLayer\n",
    "        for i in range(self.hiddenLayer):\n",
    "            for j in range(self.OutputLayer):\n",
    "                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n",
    "                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n",
    "      \n",
    "        \n",
    "        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n",
    " \n",
    "        # Atualizando pesos da HiddenLayer e InputLayer(x)\n",
    "        for i in range(self.OutputLayer):\n",
    "            for j in range(self.hiddenLayer):\n",
    "                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * x[i]))\n",
    "                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n",
    "                \n",
    "\n",
    "    def predict(self, X, y): \n",
    "       \n",
    "        my_predictions = []\n",
    "        for idx, inputs in enumerate(X): \n",
    "\n",
    "                # Forward\n",
    "            self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n",
    "            self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n",
    "\n",
    "                #One-Hot\n",
    "            if(max(self.OUTPUT_L2) == self.OUTPUT_L2[0]): \n",
    "                my_predictions.append(0)\n",
    "            else:\n",
    "                my_predictions.append(1)\n",
    "   \n",
    "        array_score = []\n",
    "        print(my_predictions)\n",
    "        mp = len(my_predictions)\n",
    "        print(mp)\n",
    "        for i in range(mp):\n",
    "          if my_predictions[i] == 0: \n",
    "                array_score.append([i, 'Não fraudulento', my_predictions[i], y[i]])\n",
    "          elif my_predictions[i] == 1:\n",
    "                array_score.append([i, 'Fraudulento', my_predictions[i], y[i]])\n",
    "                    \n",
    "        dataframe = pd.DataFrame(array_score, columns=['_id', 'class', 'output', 'hoped_output'])\n",
    "\n",
    "        return my_predictions, dataframe\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y):  \n",
    "        count_epoch = 1\n",
    "        total_error = 0\n",
    "        n = len(X); \n",
    "        epoch_array = []\n",
    "        error_array = []\n",
    "        W0 = []\n",
    "        W1 = []\n",
    "        while(count_epoch <= self.max_epochs):\n",
    "            for idx,inputs in enumerate(X): \n",
    "                self.output = np.zeros(self.classes_number)\n",
    "                \n",
    "                # Forward\n",
    "                self.OUTPUT_L1 = self.activation((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n",
    "                self.OUTPUT_L2 = self.activation((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n",
    "           \n",
    "                #One-Hot\n",
    "                if(y[idx] == 0): \n",
    "                    self.output = np.array([1,0]) \n",
    "                elif(y[idx] == 1):\n",
    "                    self.output = np.array([0,1]) \n",
    "                \n",
    "                square_error = 0\n",
    "                for i in range(self.OutputLayer):\n",
    "                    erro = (self.output[i] - self.OUTPUT_L2[i])**2\n",
    "                    square_error = (square_error + (0.05 * erro))\n",
    "                    total_error = total_error + square_error\n",
    "         \n",
    "                # Backpropagation \n",
    "                self.Backpropagation_Algorithm(inputs)\n",
    "                # Verificar o aprendizado do algoritmo\n",
    "            total_error = (total_error / n)\n",
    "            if((count_epoch % 10 == 0)or(count_epoch == 1)):\n",
    "                print(\"Epoch \", count_epoch, \"- Total error: \",total_error, f'\\n Accuracy: {round((1-total_error)*100,2)}%')\n",
    "                error_array.append(total_error)\n",
    "                epoch_array.append(count_epoch)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "            W0.append(self.WEIGHT_hidden)\n",
    "            W1.append(self.WEIGHT_output)\n",
    "             \n",
    "                \n",
    "            count_epoch += 1\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = df.columns\n",
    "     \n",
    "\n",
    "for i in colunas:\n",
    "  df[i] = (df[i] - df[i].min()) / (df[i].max() - df[i].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Total error:  0.00026339818764952697 \n",
      " Accuracy: 99.97%\n",
      "Epoch  10 - Total error:  0.00025955296021166975 \n",
      " Accuracy: 99.97%\n",
      "Epoch  20 - Total error:  0.00025954476061125904 \n",
      " Accuracy: 99.97%\n",
      "Epoch  30 - Total error:  0.00025953687456325877 \n",
      " Accuracy: 99.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[colunas.drop('Class')]\n",
    "Y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n",
    "     \n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "     \n",
    "\n",
    "dic = {'InputLayer':30, 'HiddenLayer':10, 'OutputLayer':2,\n",
    "              'Epocas':30, 'LearningRate':0.05,'BiasHiddenValue':np.random.rand(), \n",
    "              'BiasOutputValue':np.random.rand(), 'ActivationFunction':'sigmoid'}\n",
    "\n",
    "Perceptron = MultiLayerPerceptron(dic)\n",
    "Perceptron.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
