{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "data = pd.read_csv(\"winequality.csv\")\n",
    "\n",
    "# Separando as features (X) e os rótulos (y)\n",
    "X = data.drop('quality', axis=1).values\n",
    "y = data['quality'].values\n",
    "\n",
    "# Dividindo o dataset em conjunto de treinamento e teste (80% treinamento, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(X_train, X_test, y_train, k=3):\n",
    "    y_pred = []\n",
    "    for test_point in X_test:\n",
    "        # Calculando as distâncias para todos os pontos de treinamento\n",
    "        distances = [euclidean_distance(test_point, train_point) for train_point in X_train]\n",
    "\n",
    "        # Obtendo os índices dos k vizinhos mais próximos\n",
    "        k_neighbors_indices = np.argsort(distances)[:k]\n",
    "\n",
    "        # Obtendo os rótulos dos vizinhos mais próximos\n",
    "        k_neighbors_labels = [y_train[i] for i in k_neighbors_indices]\n",
    "\n",
    "        # Fazendo a previsão para o ponto de teste: a classe mais comum entre os vizinhos mais próximos\n",
    "        prediction = np.bincount(k_neighbors_labels).argmax()\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "    return np.array(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do k-NN: 0.33615384615384614\n"
     ]
    }
   ],
   "source": [
    "# Definindo o valor de k (número de vizinhos mais próximos)\n",
    "k = 3\n",
    "\n",
    "# Fazendo as previsões usando o k-NN\n",
    "y_pred_knn = k_nearest_neighbors(X_train, X_test, y_train, k)\n",
    "\n",
    "# Calculando a acurácia do k-NN\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "print(\"Acurácia do k-NN:\", accuracy_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importância das features (Random Forest):\n",
      "                  Feature  Importance\n",
      "11               alcohol    0.251824\n",
      "2       volatile acidity    0.117817\n",
      "6    free sulfur dioxide    0.082968\n",
      "10             sulphates    0.074930\n",
      "7   total sulfur dioxide    0.072466\n",
      "4         residual sugar    0.066937\n",
      "9                     pH    0.061879\n",
      "5              chlorides    0.058888\n",
      "3            citric acid    0.055962\n",
      "8                density    0.052647\n",
      "1          fixed acidity    0.052399\n",
      "0             Unnamed: 0    0.050493\n",
      "12               quality    0.000789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Criando o modelo de Random Forest\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinando o modelo com o conjunto de treinamento\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo a importância das features\n",
    "feature_importance = random_forest.feature_importances_\n",
    "\n",
    "# Criando um DataFrame para visualizar a importância das features\n",
    "importance_df = pd.DataFrame({'Feature': data.columns[:-1], 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Importância das features (Random Forest):\\n\", importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_features = importance_df['Feature'][:3].values\n",
    "\n",
    "# Filtrando o conjunto de treinamento e teste com as três features mais importantes\n",
    "X_train_top3 = X_train[:, importance_df.index[:3]]\n",
    "X_test_top3 = X_test[:, importance_df.index[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do k-NN (com as 3 features mais importantes): 0.666923076923077\n"
     ]
    }
   ],
   "source": [
    "def map_quality_label(quality):\n",
    "    # Mapear a qualidade do vinho para as categorias: 'baixa', 'média' e 'alta'\n",
    "    if quality <= 5:\n",
    "        return 'baixa'\n",
    "    elif quality <= 7:\n",
    "        return 'média'\n",
    "    else:\n",
    "        return 'alta'\n",
    "\n",
    "# Fazendo as previsões usando o k-NN nas três features mais importantes\n",
    "knn_top3 = k_nearest_neighbors(X_train_top3, X_test_top3, y_train, k)\n",
    "\n",
    "# Mapeando as previsões de qualidade para as categorias\n",
    "predicted_quality_labels = np.array([map_quality_label(quality) for quality in knn_top3])\n",
    "\n",
    "# Mapeando as categorias reais do conjunto de teste\n",
    "true_quality_labels = np.array([map_quality_label(quality) for quality in y_test])\n",
    "\n",
    "# Calculando a acurácia do k-NN com as três features mais importantes\n",
    "accuracy_knn_top3 = accuracy_score(true_quality_labels, predicted_quality_labels)\n",
    "print(\"Acurácia do k-NN (com as 3 features mais importantes):\", accuracy_knn_top3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Features True_Quality Predicted_Quality\n",
      "0    [9.9, 0.47, 36.0]        baixa             média\n",
      "1  [11.6, 0.405, 10.0]        baixa             baixa\n",
      "2   [11.3, 0.56, 24.0]        baixa             média\n",
      "3   [11.8, 0.44, 60.0]        média             média\n",
      "4     [9.6, 0.51, 8.0]        média             baixa\n"
     ]
    }
   ],
   "source": [
    "# Mostrar as primeiras observações do conjunto de teste e a nova classificação\n",
    "result_df = pd.DataFrame({\n",
    "    'Features': X_test_top3.tolist(),\n",
    "    'True_Quality': true_quality_labels,\n",
    "    'Predicted_Quality': predicted_quality_labels\n",
    "})\n",
    "\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.9930769230769231\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       979\n",
      "           1       0.99      0.98      0.99       321\n",
      "\n",
      "    accuracy                           0.99      1300\n",
      "   macro avg       0.99      0.99      0.99      1300\n",
      "weighted avg       0.99      0.99      0.99      1300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data.drop('wine_is_red', axis=1).values\n",
    "y = data['wine_is_red'].values\n",
    "\n",
    "# Dividindo o dataset em conjunto de treinamento e teste (80% treinamento, 20% teste)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando o modelo de Random Forest Classifier\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Treinando o modelo com o conjunto de treinamento\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo previsões no conjunto de teste\n",
    "y_pred = random_forest_classifier.predict(X_test)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Acurácia do modelo:\", accuracy)\n",
    "\n",
    "# Imprimindo o relatório de classificação (com métricas de precisão, recall, f1-score, etc.)\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
