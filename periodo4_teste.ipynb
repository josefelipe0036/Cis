{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Convolucionais\n",
    "\n",
    "## Convolução\n",
    "\n",
    "A convolução é um conceito fundamental em processamento de sinais e análise de imagens, frequentemente usado em áreas como processamento de imagem, visão computacional e aprendizado de máquina. É uma operação matemática que combina dois conjuntos de dados para produzir um terceiro conjunto de dados, geralmente representando uma mistura ou sobreposição das informações nos conjuntos originais. Em processamento de imagens, a convolução é usada para aplicar filtros ou máscaras a uma imagem para realizar tarefas como detecção de bordas, suavização ou realce de características. No contexto de aprendizado de máquina, a convolução é uma parte fundamental das redes neurais convolucionais (CNNs), que são amplamente utilizadas para tarefas de visão computacional, como classificação de imagens e detecção de objetos.\n",
    "\n",
    "## Diferença entre uma CNN e uma RNA tradicional \n",
    "Redes neurais e redes neurais convolucionais (CNNs) são ambas estruturas de aprendizado de máquina que consistem em várias camadas de neurônios artificiais interconectados. No entanto, elas têm diferentes arquiteturas e são projetadas para lidar com tipos diferentes de dados e tarefas. Aqui estão algumas das principais diferenças entre as duas:\n",
    "\n",
    "### Arquitetura e Conectividade:\n",
    "\n",
    "Redes Neurais (RNs): As redes neurais tradicionais, também conhecidas como perceptrons multicamadas, consistem em camadas de neurônios interconectados. Cada neurônio em uma camada está conectado a todos os neurônios da camada anterior e da próxima camada.\n",
    "Redes Neurais Convolucionais (CNNs): As CNNs foram projetadas especificamente para processar dados de grade, como imagens. Elas incluem camadas de convolução que aplicam operações de convolução a regiões locais das entradas, permitindo a detecção de padrões locais. Além disso, as camadas de pooling são usadas para reduzir a dimensionalidade da representação e capturar características invariantes a pequenas traduções.\n",
    "Processamento de Dados:\n",
    "\n",
    "RNs: Embora as redes neurais possam ser usadas para uma variedade de tarefas, como classificação de texto e séries temporais, elas não são otimizadas para dados com estrutura espacial, como imagens.\n",
    "CNNs: As CNNs são especialmente adequadas para tarefas de visão computacional devido à sua capacidade de explorar a estrutura espacial das imagens. Elas usam filtros convolucionais para detectar padrões como bordas, texturas e formas em diferentes regiões da imagem.\n",
    "Parâmetros Compartilhados:\n",
    "\n",
    "RNs: Em redes neurais tradicionais, cada neurônio em uma camada está conectado a todos os neurônios da camada anterior, resultando em muitos parâmetros treináveis.\n",
    "CNNs: Nas camadas de convolução das CNNs, os filtros são compartilhados em várias regiões da entrada. Isso reduz significativamente o número de parâmetros e permite que as CNNs capturem eficientemente características locais em diferentes partes da imagem.\n",
    "Invariância Espacial:\n",
    "\n",
    "RNs: Redes neurais tradicionais não são projetadas para lidar explicitamente com a invariância espacial, o que é importante em tarefas de visão computacional.\n",
    "CNNs: Devido à aplicação de convoluções e camadas de pooling, as CNNs são capazes de aprender características que são invariantes a pequenas variações de posição na imagem.\n",
    "Aplicações:\n",
    "\n",
    "RNs: São usadas para uma ampla gama de tarefas, incluindo processamento de linguagem natural, análise de séries temporais e outras tarefas que não envolvem explicitamente dados de grade.\n",
    "CNNs: São especialmente adequadas para tarefas de visão computacional, como classificação de imagens, detecção de objetos, segmentação semântica e muito mais.\n",
    "Em resumo, as redes neurais convolucionais (CNNs) foram projetadas especificamente para lidar com dados de grade, como imagens, e são altamente eficazes em tarefas de visão computacional devido à sua capacidade de capturar informações espaciais e hierárquicas nas imagens. Por outro lado, redes neurais tradicionais (RNs) são mais versáteis e podem ser usadas para uma variedade de tarefas de aprendizado de máquina, incluindo processamento de linguagem natural e análise de séries temporais.\n",
    "\n",
    "## Como são as CNNs\n",
    "\n",
    "\n",
    "Uma Convolutional Neural Network (CNN), é uma arquitetura especializada em processamento de imagens e dados com estrutura espacial. Ela consiste em várias camadas que operam de maneira hierárquica para extrair características relevantes de uma imagem e, em seguida, usá-las para realizar tarefas como classificação, detecção de objetos, segmentação, entre outras. Abaixo está uma explicação passo a passo de como uma CNN é construída:\n",
    "\n",
    "### 1. Camada de Entrada\n",
    "\n",
    "A entrada para a CNN é a imagem que você deseja processar. Uma imagem é uma matriz tridimensional de pixels, onde as dimensões são largura, altura e canais de cor (como vermelho, verde e azul em uma imagem RGB).\n",
    "\n",
    "### 2. Camadas de Convolução\n",
    "\n",
    "As camadas de convolução são o cerne de uma CNN. Cada camada consiste em vários filtros (também chamados de kernels) que deslizam pela imagem para realizar a operação de convolução. Esses filtros são responsáveis por detectar padrões e características específicas, como bordas, texturas e formas. À medida que os filtros são aplicados, eles produzem mapas de características que representam a presença dessas características na imagem.\n",
    "\n",
    "### 3. Camadas de Ativação\n",
    "\n",
    "Após a operação de convolução, é comum aplicar uma função de ativação, como a função ReLU (Rectified Linear Activation), para introduzir não-linearidade nas saídas das camadas. Isso permite que a rede aprenda relações mais complexas entre as características.\n",
    "\n",
    "### 4. Camadas de Pooling\n",
    "\n",
    "As camadas de pooling são usadas para reduzir a dimensionalidade da representação, diminuindo a quantidade de cálculos necessários e tornando a rede mais eficiente. O pooling mais comum é o Max Pooling, onde apenas o valor máximo em uma região é mantido, descartando os outros valores. Isso também ajuda a tornar a rede mais robusta a pequenas variações na posição dos objetos na imagem.\n",
    "\n",
    "### 5. Camadas Totalmente Conectadas\n",
    "\n",
    "Após passar por várias camadas de convolução e pooling, os mapas de características resultantes são achatados em um vetor unidimensional e alimentados em camadas totalmente conectadas, semelhantes às camadas em redes neurais tradicionais. Essas camadas finais da CNN combinam as informações aprendidas das características em uma representação que pode ser usada para a tarefa específica, como classificação ou detecção.\n",
    "\n",
    "### 6. Camada de Saída\n",
    "\n",
    "A camada de saída produz a saída final da rede, que pode variar de acordo com a tarefa. Por exemplo, em uma tarefa de classificação, pode ser um vetor de probabilidades associadas a diferentes classes. Em detecção de objetos, pode ser um conjunto de caixas delimitadoras e as classes correspondentes.\n",
    "\n",
    "### 7. Treinamento\n",
    "\n",
    "A CNN é treinada usando um algoritmo de otimização, como o gradiente descendente, para ajustar os pesos das conexões entre os neurônios. O treinamento é feito comparando as previsões da rede com os rótulos verdadeiros e ajustando os pesos para minimizar o erro.\n",
    "\n",
    "### 8. Camadas de Regularização e Normalização\n",
    "\n",
    "Camadas adicionais, como a camada de dropout para regularização e a camada de normalização por lotes, podem ser adicionadas para melhorar o desempenho e a generalização da rede.\n",
    "\n",
    "Em resumo, uma CNN é construída usando camadas de convolução, ativação, pooling e totalmente conectadas, com o objetivo de extrair e combinar características em uma hierarquia para tarefas de processamento de imagens. A arquitetura exata e o número de camadas dependem da complexidade da tarefa e da natureza dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def prepare_directories(data_dir, target_dir):\n",
    "    # Crie um diretório temporário para consolidar os dados\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # Copie as pastas de classes para o diretório temporário\n",
    "    classes = ['cats', 'dogs', 'panda']\n",
    "    for class_name in classes:\n",
    "        source_path = os.path.join(data_dir, class_name)\n",
    "        target_path = os.path.join(temp_dir, class_name)\n",
    "        shutil.copytree(source_path, target_path)\n",
    "    \n",
    "    # Copie os dados do diretório temporário para o diretório alvo\n",
    "    shutil.rmtree(target_dir, ignore_errors=True)\n",
    "    shutil.move(temp_dir, target_dir)\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "# Defina o caminho absoluto para a pasta contendo as imagens\n",
    "data_dir = os.path.join(home_dir, 'UnB/CIS/animals')\n",
    "\n",
    "target_dir = os.path.join(home_dir, 'UnB/CIS/teste')\n",
    "\n",
    "# Prepare os diretórios\n",
    "prepare_directories(data_dir, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Defina os parâmetros\n",
    "image_size = (224, 224)\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "# Crie um gerador de dados\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Carregue os dados de treinamento e validação\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    target_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    target_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Crie o modelo da rede convolucional\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile o modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "240/240 [==============================] - 355s 1s/step - loss: 0.8773 - accuracy: 0.5738 - val_loss: 0.7642 - val_accuracy: 0.6050\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 381s 2s/step - loss: 0.7106 - accuracy: 0.6538 - val_loss: 0.7701 - val_accuracy: 0.5983\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 366s 2s/step - loss: 0.6130 - accuracy: 0.7175 - val_loss: 0.9461 - val_accuracy: 0.5800\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 372s 2s/step - loss: 0.4818 - accuracy: 0.7771 - val_loss: 0.8240 - val_accuracy: 0.6500\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 363s 2s/step - loss: 0.3676 - accuracy: 0.8429 - val_loss: 0.9481 - val_accuracy: 0.6183\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 389s 2s/step - loss: 0.2774 - accuracy: 0.8821 - val_loss: 1.1973 - val_accuracy: 0.6333\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 332s 1s/step - loss: 0.1948 - accuracy: 0.9267 - val_loss: 1.4775 - val_accuracy: 0.6450\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 368s 2s/step - loss: 0.1252 - accuracy: 0.9554 - val_loss: 1.6306 - val_accuracy: 0.6317\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 382s 2s/step - loss: 0.1006 - accuracy: 0.9679 - val_loss: 1.7640 - val_accuracy: 0.6233\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 407s 2s/step - loss: 0.0556 - accuracy: 0.9804 - val_loss: 2.2384 - val_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Treine o modelo\n",
    "history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "#model.save('modelo_treinado.h5')\n",
    "\n",
    "# Salve também o histórico de treinamento em um arquivo JSON\n",
    "import json\n",
    "history_dict = history.history\n",
    "with open('history.json', 'w') as json_file:\n",
    "    json.dump(history_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Carregue o modelo treinado\n",
    "loaded_model = load_model('modelo_treinado.h5')\n",
    "\n",
    "# Leia o histórico de treinamento de volta\n",
    "with open('history.json', 'r') as json_file:\n",
    "    loaded_history_dict = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               44302848  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44397635 (169.36 MB)\n",
      "Trainable params: 44397635 (169.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()#parametros do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "240/240 [==============================] - 934s 4s/step - loss: 1.1019 - accuracy: 0.7342 - val_loss: 0.3279 - val_accuracy: 0.8883\n",
      "Epoch 2/10\n",
      "240/240 [==============================] - 859s 4s/step - loss: 0.3379 - accuracy: 0.8633 - val_loss: 0.3590 - val_accuracy: 0.8650\n",
      "Epoch 3/10\n",
      "240/240 [==============================] - 751s 3s/step - loss: 0.2603 - accuracy: 0.8967 - val_loss: 0.3800 - val_accuracy: 0.8700\n",
      "Epoch 4/10\n",
      "240/240 [==============================] - 712s 3s/step - loss: 0.2205 - accuracy: 0.9071 - val_loss: 0.2927 - val_accuracy: 0.8800\n",
      "Epoch 5/10\n",
      "240/240 [==============================] - 874s 4s/step - loss: 0.1981 - accuracy: 0.9254 - val_loss: 0.2986 - val_accuracy: 0.8950\n",
      "Epoch 6/10\n",
      "240/240 [==============================] - 882s 4s/step - loss: 0.1852 - accuracy: 0.9217 - val_loss: 0.4953 - val_accuracy: 0.8533\n",
      "Epoch 7/10\n",
      "240/240 [==============================] - 950s 4s/step - loss: 0.1498 - accuracy: 0.9392 - val_loss: 0.3187 - val_accuracy: 0.8967\n",
      "Epoch 8/10\n",
      "240/240 [==============================] - 901s 4s/step - loss: 0.1617 - accuracy: 0.9408 - val_loss: 0.3541 - val_accuracy: 0.8833\n",
      "Epoch 9/10\n",
      "240/240 [==============================] - 924s 4s/step - loss: 0.1274 - accuracy: 0.9517 - val_loss: 0.3264 - val_accuracy: 0.8983\n",
      "Epoch 10/10\n",
      "240/240 [==============================] - 913s 4s/step - loss: 0.1397 - accuracy: 0.9429 - val_loss: 0.3160 - val_accuracy: 0.8933\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Carregue o modelo pré-treinado VGG16 (sem as camadas fully connected no topo)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congele as camadas do modelo base\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Crie o modelo completo\n",
    "model_transfer = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile o modelo\n",
    "model_transfer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treine o modelo\n",
    "history_transfer = model_transfer.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reviewerID', 'asin', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'overall', 'summary', 'unixReviewTime', 'reviewTime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Carregando a base de dados\n",
    "data = pd.read_csv(\"amazon.csv\")\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lemmatizer\u001b[39m.\u001b[39mlemmatize(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit()])\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n\u001b[0;32m---> 20\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mcleaned_text_spacy\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mText\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mapply(prepare_text_spacy)\n\u001b[1;32m     21\u001b[0m df \u001b[39m=\u001b[39m df[df[\u001b[39m\"\u001b[39m\u001b[39mcleaned_text_spacy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m!=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# remove empty text\u001b[39;00m\n\u001b[1;32m     23\u001b[0m X_spacy \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mcleaned_text_spacy\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English tokenizer and language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df = pd.read_csv(\"amazon.csv\")\n",
    "def prepare_text_spacy(text):\n",
    "    text = text.replace(\".\", \". \").replace(\"!\", \". \")\n",
    "    text = re.sub(r'[^a-zA-z\\s]','',text) # remove special characters\n",
    "    text = \" \".join([word.text.lower() for word in nlp(text) if word.text.lower() not in stop_words])\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(token) for token in text.split()])\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_text_spacy\"] = df[\"Text\"].apply(prepare_text_spacy)\n",
    "df = df[df[\"cleaned_text_spacy\"]!=\"\"] # remove empty text\n",
    "\n",
    "X_spacy = df[\"cleaned_text_spacy\"]\n",
    "y = df[\"overall\"]\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf_spacy = vectorizer.fit_transform(X_spacy)\n",
    "\n",
    "print(X_tfidf_spacy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jose/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Carregar o modelo do SpaCy para processamento de texto\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download dos stopwords do NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remover pontuações usando RegEx\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenização e lemmatização usando SpaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Remover stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Juntar os tokens novamente em um texto processado\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bow(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(texts)\n",
    "    return bow_matrix, vectorizer\n",
    "\n",
    "def tokenize_with_tfidf(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que 'data' seja uma lista contendo seus textos de review\n",
    "preprocessed_texts = [preprocess_text(text) for text in data]\n",
    "\n",
    "# Tokenização usando BoW\n",
    "bow_matrix, bow_vectorizer = tokenize_with_bow(preprocessed_texts)\n",
    "\n",
    "# Tokenização usando TF-IDF\n",
    "tfidf_matrix, tfidf_vectorizer = tokenize_with_tfidf(preprocessed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#################\n",
    "################\n",
    "#DEU CERTO\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Verificar se é uma string\n",
    "        # Converter para minúsculas\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remover pontuações\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Tokenização\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Remover stopwords\n",
    "        tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "        # Reunir tokens em texto novamente\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "# Aplicar pré-processamento e criar nova coluna\n",
    "data['cleaned_reviewText'] = data['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Criar matriz de features BOW\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow_features = bow_vectorizer.fit_transform(data['cleaned_reviewText'])\n",
    "\n",
    "# Criar matriz de features TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_reviewText'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.6867998051631758\n",
      "SVM Accuracy: 0.6829030686799805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Suponha que 'data' contém o DataFrame com as colunas relevantes\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Usar vetorização BOW ou TF-IDF\n",
    "vectorizer = CountVectorizer()  # Você pode alternar para TfidfVectorizer se desejar\n",
    "X_train = vectorizer.fit_transform(train_data['cleaned_reviewText'])\n",
    "X_test = vectorizer.transform(test_data['cleaned_reviewText'])\n",
    "\n",
    "# Labels\n",
    "y_train = train_data['overall']\n",
    "y_test = test_data['overall']\n",
    "\n",
    "# Treinar o classificador Naive Bayes\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "\n",
    "# Treinar o classificador SVM\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 65s 489ms/step - loss: 0.9854 - accuracy: 0.6698 - val_loss: 0.9559 - val_accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 62s 479ms/step - loss: 0.9610 - accuracy: 0.6750 - val_loss: 0.9593 - val_accuracy: 0.6810\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 60s 468ms/step - loss: 0.9613 - accuracy: 0.6750 - val_loss: 0.9554 - val_accuracy: 0.6810\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 61s 472ms/step - loss: 0.9591 - accuracy: 0.6750 - val_loss: 0.9556 - val_accuracy: 0.6810\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 61s 471ms/step - loss: 0.9585 - accuracy: 0.6750 - val_loss: 0.9554 - val_accuracy: 0.6810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd924075cf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Suponha que 'data' contém o DataFrame com as colunas relevantes\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['cleaned_reviewText'])\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_data['cleaned_reviewText'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_data['cleaned_reviewText'])\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = max(map(len, X_train_sequences))\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['overall'])\n",
    "y_train_encoded = label_encoder.transform(train_data['overall'])\n",
    "y_test_encoded = label_encoder.transform(test_data['overall'])\n",
    "\n",
    "# Criar o modelo RNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 174s 1s/step - loss: 1.0240 - accuracy: 0.6689 - val_loss: 0.9568 - val_accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 153s 1s/step - loss: 0.9622 - accuracy: 0.6750 - val_loss: 0.9672 - val_accuracy: 0.6810\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 130s 1s/step - loss: 0.9600 - accuracy: 0.6750 - val_loss: 0.9557 - val_accuracy: 0.6810\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 131s 1s/step - loss: 0.9586 - accuracy: 0.6750 - val_loss: 0.9537 - val_accuracy: 0.6810\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 131s 1s/step - loss: 0.9595 - accuracy: 0.6750 - val_loss: 0.9538 - val_accuracy: 0.6810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd92a090490>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Suponha que 'data' contém o DataFrame com as colunas relevantes\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['cleaned_reviewText'])\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_data['cleaned_reviewText'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_data['cleaned_reviewText'])\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = max(map(len, X_train_sequences))\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['overall'])\n",
    "y_train_encoded = label_encoder.transform(train_data['overall'])\n",
    "y_test_encoded = label_encoder.transform(test_data['overall'])\n",
    "\n",
    "# Criar o modelo LSTM unidirecional\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "129/129 [==============================] - 269s 2s/step - loss: 0.9972 - accuracy: 0.6676 - val_loss: 0.8912 - val_accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "129/129 [==============================] - 241s 2s/step - loss: 0.7957 - accuracy: 0.6972 - val_loss: 0.9004 - val_accuracy: 0.6717\n",
      "Epoch 3/5\n",
      "129/129 [==============================] - 241s 2s/step - loss: 0.6288 - accuracy: 0.7563 - val_loss: 0.9246 - val_accuracy: 0.6391\n",
      "Epoch 4/5\n",
      "129/129 [==============================] - 258s 2s/step - loss: 0.5196 - accuracy: 0.8067 - val_loss: 1.0487 - val_accuracy: 0.6410\n",
      "Epoch 5/5\n",
      "129/129 [==============================] - 217s 2s/step - loss: 0.4196 - accuracy: 0.8454 - val_loss: 1.2143 - val_accuracy: 0.6264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd916b373d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Suponha que 'data' contém o DataFrame com as colunas relevantes\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['cleaned_reviewText'])\n",
    "X_train_sequences = tokenizer.texts_to_sequences(train_data['cleaned_reviewText'])\n",
    "X_test_sequences = tokenizer.texts_to_sequences(test_data['cleaned_reviewText'])\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = max(map(len, X_train_sequences))\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_data['overall'])\n",
    "y_train_encoded = label_encoder.transform(train_data['overall'])\n",
    "y_test_encoded = label_encoder.transform(test_data['overall'])\n",
    "\n",
    "# Criar o modelo LSTM bidirecional\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train_padded, y_train_encoded, validation_data=(X_test_padded, y_test_encoded), epochs=5, batch_size=64)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
